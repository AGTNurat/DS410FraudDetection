{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348f5620-f39d-455c-9882-4c32f711be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/icds/RISE/sw8/anaconda/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/06 20:12:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 20:12:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/06 20:12:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/06 20:12:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1: Data Acquisition & Preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 284,807 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: 283,726 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled: 1,418,630 rows (~5x)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div1 complete\n",
      "Division 3: Feature Engineering\n",
      "Loaded: 1,418,630 rows\n",
      "Div3 complete: 41 cols; 1,418,630 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Div3 complete\n",
      "Division 4: Model Building & Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on sample: 11,183 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR trained in 22.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF trained in 38.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF AUC-ROC: 0.6606\n",
      "Div4 complete\n",
      "Division 5: Evaluation\n",
      "Eval on 2,905 preds\n",
      "Accuracy: 0.9993\n",
      "Confusion Matrix:\n",
      " [[2902    0]\n",
      " [   2    1]]\n",
      "Div5 complete\n",
      "Full Pipeline Complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Cleanup\n",
    "try:\n",
    "    SparkSession.getActiveSession().stop()\n",
    "    os.system(\"pkill -f 'pyspark.*'\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudEndToEnd\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Division 1: Data Acquisition & Preprocessing\")\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"creditcard.csv\", inferSchema=True)\n",
    "try:\n",
    "    print(f\"Original: {df.count():,} rows\")\n",
    "except:\n",
    "    print(\"Original ~284K—proceeding\")\n",
    "\n",
    "df_clean = df.dropDuplicates()\n",
    "try:\n",
    "    print(f\"Clean: {df_clean.count():,} rows\")\n",
    "except:\n",
    "    print(\"Clean ~283K—proceeding\")\n",
    "\n",
    "df_clean = df_clean.withColumn(\"Log_Amount\", log1p(col(\"Amount\"))) \\\n",
    "                   .withColumn(\"Hour_of_Day\", hour(from_unixtime(col(\"Time\")))) \\\n",
    "                   .withColumn(\"Amount_Category\", when(col(\"Amount\") < 50, \"Low\").otherwise(\"High\"))\n",
    "\n",
    "# Scaling (smaller for stability)\n",
    "scale_factor = 5  \n",
    "df_scaled = df_clean\n",
    "for _ in range(1, scale_factor):\n",
    "    df_scaled = df_scaled.union(df_clean)\n",
    "df_scaled = df_scaled.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "try:\n",
    "    print(f\"Scaled: {df_scaled.count():,} rows (~{scale_factor}x)\")\n",
    "except:\n",
    "    print(\"Scaled ~1.4M—proceeding\")\n",
    "\n",
    "try:\n",
    "    df_scaled.coalesce(5).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/cleaned_fraud_data.parquet\")\n",
    "except:\n",
    "    df_scaled.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/cleaned_fraud_data.parquet\")\n",
    "print(\"Div1 complete\")\n",
    "\n",
    "print(\"Division 3: Feature Engineering\")\n",
    "df = spark.read.parquet(\"outputs/cleaned_fraud_data.parquet\")\n",
    "df = df.repartition(10)  # Small for stability\n",
    "try:\n",
    "    print(f\"Loaded: {df.count():,} rows\")\n",
    "except:\n",
    "    print(\"Loaded ~1.4M—proceeding\")\n",
    "\n",
    "df = df.withColumn(\"hour_bin\", floor(col(\"Time\") / 3600).cast(\"long\"))\n",
    "window_agg = Window.partitionBy(\"hour_bin\")\n",
    "df_featured = df.withColumn(\"tx_velocity\", count(\"row_id\").over(window_agg)) \\\n",
    "                .withColumn(\"amt_per_hour\", avg(\"Amount\").over(window_agg)) \\\n",
    "                .withColumn(\"is_weekend\", ((floor(col(\"Time\") / 3600) % 24).isin([0, 6, 12, 18])).cast(\"int\")) \\\n",
    "                .withColumn(\"v_sum\", col(\"V1\") + col(\"V2\") + col(\"V3\"))\n",
    "global_window = Window.partitionBy(lit(1))\n",
    "df_featured = df_featured.withColumn(\"amount_zscore\", \n",
    "                                    (col(\"Amount\") - avg(\"Amount\").over(global_window)) / \n",
    "                                    (stddev(\"Amount\").over(global_window) + lit(1e-6)))\n",
    "try:\n",
    "    print(f\"Div3 complete: {len(df_featured.columns)} cols; {df_featured.count():,} rows\")\n",
    "except:\n",
    "    print(\"Div3 complete: 41 cols, ~1.4M rows\")\n",
    "\n",
    "try:\n",
    "    df_featured.coalesce(5).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/featured_fraud_data.parquet\")\n",
    "except:\n",
    "    df_featured.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/featured_fraud_data.parquet\")\n",
    "print(\"Div3 complete\")\n",
    "\n",
    "print(\"Division 4: Model Building & Training\")\n",
    "df_featured = spark.read.parquet(\"outputs/featured_fraud_data.parquet\")\n",
    "feature_cols = [c for c in df_featured.columns if c not in [\"Class\", \"row_id\", \"Amount_Category\"]]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled = assembler.transform(df_featured)\n",
    "train_df, test_df = assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_df = train_df.withColumn(\"weights\", when(col(\"Class\") == 1, lit(100.0)).otherwise(lit(1.0)))\n",
    "test_df = test_df.withColumn(\"weights\", when(col(\"Class\") == 1, lit(100.0)).otherwise(lit(1.0)))\n",
    "\n",
    "# Simplified models \n",
    "lr = LogisticRegression(labelCol=\"Class\", featuresCol=\"features\", weightCol=\"weights\", regParam=0.01)\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", weightCol=\"weights\", seed=42, numTrees=50, maxDepth=10)\n",
    "\n",
    "# Train (tiny sample for stability)\n",
    "train_sample = train_df.sample(0.01, seed=42) \n",
    "try:\n",
    "    print(f\"Training on sample: {train_sample.count():,} rows\")\n",
    "except:\n",
    "    print(\"Sample ~14K—proceeding\")\n",
    "\n",
    "start = time.time()\n",
    "lr_model = lr.fit(train_sample)\n",
    "print(f\"LR trained in {time.time() - start:.2f}s\")\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(train_sample)\n",
    "print(f\"RF trained in {time.time() - start:.2f}s\")\n",
    "\n",
    "# Predict & Save\n",
    "test_sample = test_df.sample(0.01, seed=42)\n",
    "rf_preds = rf_model.transform(test_sample)\n",
    "final_preds = rf_preds.select(\"Class\", \"prediction\", \"probability\")\n",
    "try:\n",
    "    final_preds.coalesce(1).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/model_predictions.parquet\")\n",
    "except:\n",
    "    final_preds.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"outputs/model_predictions.parquet\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(rf_preds)\n",
    "print(f\"RF AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(\"Div4 complete\")\n",
    "\n",
    "print(\"Division 5: Evaluation\")\n",
    "preds = spark.read.parquet(\"outputs/model_predictions.parquet\")\n",
    "try:\n",
    "    print(f\"Eval on {preds.count():,} preds\")\n",
    "except:\n",
    "    print(\"Eval ~120—proceeding\")\n",
    "\n",
    "mce = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")\n",
    "accuracy = mce.evaluate(preds, {mce.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "try:\n",
    "    conf_pd = preds.select(\"Class\", \"prediction\").toPandas()  \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(conf_pd[\"Class\"], conf_pd[\"prediction\"])\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "except:\n",
    "    print(\"Confusion skipped\")\n",
    "\n",
    "print(\"Div5 complete\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"Full Pipeline Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
