{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc443234-0fc2-46fb-8e0a-923a5c0b18de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/icds/RISE/sw8/anaconda/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/06 19:08:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 19:08:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/06 19:08:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/06 19:08:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh session created with Kryo serializer.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import warnings\n",
    "\n",
    "# Stop any existing session first (safety)\n",
    "try:\n",
    "    SparkSession.getActiveSession().stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Build fresh session with all configs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Div4_Modeling\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Fresh session created with Kryo serializer.\") #for faster serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ffa3a4-8632-4f64-9aec-88112ba9a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kryo active: True\n",
      "Driver memory: 4g\n"
     ]
    }
   ],
   "source": [
    "print(\"Kryo active:\", spark.conf.get(\"spark.serializer\") == \"org.apache.spark.serializer.KryoSerializer\")\n",
    "print(\"Driver memory:\", spark.conf.get(\"spark.driver.memory\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9b66f-a39e-4c70-948f-a13575d13189",
   "metadata": {},
   "source": [
    "## Division 4: Data Preparation (From Div3 Output)\n",
    "\n",
    "Load featured data (41 cols, 6M rows). Assemble 37 numeric features into vectors (exclude Class, row_id, Amount_Category). Split 80/20 for train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5a21e1-e00f-4f02-849a-93525aa7ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5,998,034 rows from featured data\n",
      "Selected 38 features: ['Time', 'V1', 'V2', 'V3', 'V4']...\n",
      "Features assembled into DenseVectors\n"
     ]
    }
   ],
   "source": [
    "# load featured data from Div3\n",
    "df_featured = spark.read.parquet(\"featured_fraud_data.parquet\")  \n",
    "print(f\"Loaded {df_featured.count():,} rows from featured data\")\n",
    "\n",
    "# Select numeric features \n",
    "feature_cols = [c for c in df_featured.columns if c not in [\"Class\", \"row_id\", \"Amount_Category\"]]\n",
    "print(f\"Selected {len(feature_cols)} features: {feature_cols[:5]}...\")  # Preview first 5\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "assembled = assembler.transform(df_featured)\n",
    "\n",
    "print(\"Features assembled into DenseVectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2dac2e-eea8-44e1-a795-54659b4e9129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled rows: 5998034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 4797417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test rows: 1200617\n",
      "Features dim: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|                                          features|Class|\n",
      "+--------------------------------------------------+-----+\n",
      "|[2.0,-0.425965884412454,0.960523044882985,1.141...|    0|\n",
      "|[10.0,0.38497821518095,0.616109459176472,-0.874...|    0|\n",
      "|[59.0,-0.773292609110981,-4.1460072502577,-0.93...|    0|\n",
      "|[60.0,1.10702937694843,0.216441000371351,0.5383...|    0|\n",
      "|[275.0,-0.363518693983855,0.0554644052427988,1....|    0|\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split (80/20)\n",
    "train_df, test_df = assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# check shapes\n",
    "print(\"Assembled rows:\", assembled.count())\n",
    "print(\"Train rows:\", train_df.count())\n",
    "print(\"Test rows:\", test_df.count())\n",
    "print(\"Features dim:\", len(feature_cols))\n",
    "\n",
    "# sample vectors \n",
    "train_df.select(\"features\", \"Class\").sample(0.001).show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40869861-f5cd-4431-b310-dae47f0a4b6e",
   "metadata": {},
   "source": [
    "## Model Building: LR & RF with Imbalance Handling\n",
    "\n",
    "Logistic Regression (linear baseline) + Random Forest (ensemble for nonlinearity). Class weights (100x fraud) to address 0.17% imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98f2908-7918-4f52-a326-7872a09dbc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready with weights for imbalance.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "import time\n",
    "\n",
    "# add weights \n",
    "train_df = train_df.withColumn(\"weights\", when(col(\"Class\") == 1, lit(100.0)).otherwise(lit(1.0)))\n",
    "test_df = test_df.withColumn(\"weights\", when(col(\"Class\") == 1, lit(100.0)).otherwise(lit(1.0)))\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression(labelCol=\"Class\", featuresCol=\"features\", weightCol=\"weights\")\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", weightCol=\"weights\", seed=42, numTrees=50)\n",
    "\n",
    "print(\"Models ready with weights for imbalance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ed4da4-f023-4572-a598-1d0d8b3b69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning setups ready.\n"
     ]
    }
   ],
   "source": [
    "# Grids \n",
    "lr_param_grid = (ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1]).build())\n",
    "rf_param_grid = (ParamGridBuilder().addGrid(rf.maxDepth, [5, 10]).addGrid(rf.numTrees, [20, 50]).build())\n",
    "\n",
    "# Evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# CV\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_param_grid, evaluator=evaluator, numFolds=3, seed=42, parallelism=2)\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_param_grid, evaluator=evaluator, numFolds=3, seed=42, parallelism=2)\n",
    "\n",
    "# Pipelines\n",
    "lr_pipeline = Pipeline(stages=[lr_cv])\n",
    "rf_pipeline = Pipeline(stages=[rf_cv])\n",
    "\n",
    "print(\"Tuning setups ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc191d7-25f7-4187-a9f9-5b5df4a814ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample: 479,195 train rows (full: 4,797,417)\n",
      "Training LR CV (parallelism = 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR trained in 228.53s\n",
      "Training RF CV (parallelism = 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF trained in 413.77s\n",
      "Best LR regParam: 0.01\n",
      "Fallback models used: no tuned params.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# sample for dev \n",
    "train_df_sample = train_df.sample(0.1, seed=42)  \n",
    "print(f\"Using sample: {train_df_sample.count():,} train rows (full: {train_df.count():,})\")\n",
    "\n",
    "# Train LR (parallelism=1 to avoid network issues)\n",
    "start = time.time()\n",
    "print(\"Training LR CV (parallelism = 1)...\")\n",
    "try:\n",
    "    lr_model = lr_pipeline.copy({lr_cv.parallelism: 1}).fit(train_df_sample)  # Copy to override\n",
    "    lr_time = time.time() - start\n",
    "    print(f\"LR trained in {lr_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"LR retry with no CV: {e}\")\n",
    "    # Fallback: No tuning\n",
    "    lr_fallback = lr.fit(train_df_sample)\n",
    "    lr_model = Pipeline(stages=[lr_fallback])  # Wrap as \"model\"\n",
    "\n",
    "# Train RF (same fix)\n",
    "start = time.time()\n",
    "print(\"Training RF CV (parallelism = 1)...\")\n",
    "try:\n",
    "    rf_model = rf_pipeline.copy({rf_cv.parallelism: 1}).fit(train_df_sample)\n",
    "    rf_time = time.time() - start\n",
    "    print(f\"RF trained in {rf_time:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"RF retry with no CV: {e}\")\n",
    "    rf_fallback = rf.fit(train_df_sample)\n",
    "    rf_model = Pipeline(stages=[rf_fallback])\n",
    "\n",
    "# Best params (or fallback)\n",
    "try:\n",
    "    lr_best = lr_model.stages[0].bestModel\n",
    "    rf_best = rf_model.stages[0].bestModel\n",
    "    print(\"Best LR regParam:\", lr_best.getRegParam())\n",
    "    print(\"Best RF maxDepth:\", rf_best.getMaxDepth(), \"| numTrees:\", rf_best.getNumTrees())\n",
    "except:\n",
    "    print(\"Fallback models used: no tuned params.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2805dc07-220b-48f8-adb1-7e3f1327e6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test sample: 120,159 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF predictions saved to model_predictions.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Test AUC-ROC: 0.9901\n",
      "RF Test AUC-ROC: 0.9971 (target >0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 693:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|Class|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0    |0.0       |[0.9959822097198483,0.004017790280151658] |\n",
      "|0    |0.0       |[0.9959822097198483,0.004017790280151658] |\n",
      "|0    |0.0       |[0.9945413990238231,0.0054586009761769146]|\n",
      "|0    |0.0       |[0.9935353229972833,0.006464677002716721] |\n",
      "|0    |0.0       |[0.9952072756880055,0.004792724311994559] |\n",
      "|0    |0.0       |[0.9884496524642982,0.011550347535701748] |\n",
      "|0    |0.0       |[0.9952072756880055,0.004792724311994559] |\n",
      "|0    |0.0       |[0.9959296572004501,0.004070342799549848] |\n",
      "|0    |0.0       |[0.9828800727722535,0.017119927227746412] |\n",
      "|0    |0.0       |[0.9943544160564934,0.005645583943506532] |\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predictions on test \n",
    "\n",
    "test_sample = test_df.sample(0.1, seed=42) if 'test_df_sample' not in locals() else test_df  \n",
    "print(f\"Using test sample: {test_sample.count():,} rows\")\n",
    "\n",
    "# Predict\n",
    "lr_preds = lr_model.transform(test_sample)\n",
    "rf_preds = rf_model.transform(test_sample)\n",
    "\n",
    "# Final: RF preds \n",
    "final_preds = rf_preds.select(\"Class\", \"prediction\", \"probability\", \"rawPrediction\", \"weights\")\n",
    "\n",
    "# Save Parquet for Div5\n",
    "final_preds.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\"model_predictions.parquet\")\n",
    "print(\"RF predictions saved to model_predictions.parquet\")\n",
    "\n",
    "# Quick Eval (AUC-ROC for imbalance)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "lr_auc = evaluator.evaluate(lr_preds)\n",
    "rf_auc = evaluator.evaluate(rf_preds)\n",
    "print(f\"LR Test AUC-ROC: {lr_auc:.4f}\")\n",
    "print(f\"RF Test AUC-ROC: {rf_auc:.4f} (target >0.95)\")\n",
    "\n",
    "# Sample predictions\n",
    "final_preds.select(\"Class\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58d9f9-b4c9-41b3-8153-9226874a9939",
   "metadata": {},
   "source": [
    "## Div4 Summary\n",
    "- Trained: LR (reg=0.01, AUC ~0.99) + RF (fallback defaults, AUC ~0.95+) on 479K sample.\n",
    "- Imbalance: Weights effective, check fraud recall in Div5.\n",
    "- Scalability: 10min local; full data/cluster for prod.\n",
    "- Output: model_predictions.parquet ready for eval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
