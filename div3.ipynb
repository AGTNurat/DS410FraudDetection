{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e05b986-6898-4723-9025-2b86918de67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/06 18:37:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/06 18:37:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/06 18:37:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5998034"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Div3_Features\").getOrCreate()\n",
    "df = spark.read.parquet(\"cleaned_fraud_data.parquet\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f05be8-73f8-4b07-8975-aa9e6a08660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 18:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/06 18:38:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/06 18:38:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/06 18:38:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/06 18:38:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/12/06 18:38:54 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 180.2 MiB so far)\n",
      "25/12/06 18:38:54 WARN BlockManager: Persisting block rdd_19_0 to disk instead.\n",
      "25/12/06 18:39:36 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n",
      "25/12/06 18:39:37 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete! New columns added: hour_bin, tx_velocity, amt_per_hour, is_weekend, v_sum, amount_zscore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Manual hour binning to avoid interval parsing \n",
    "df = df.withColumn(\"hour_bin\", floor(col(\"Time\") / 3600).cast(\"long\"))  \n",
    "\n",
    "# Window for per hour aggregates\n",
    "window_agg = Window.partitionBy(\"hour_bin\")\n",
    "\n",
    "# New features \n",
    "df_featured = df.withColumn(\"tx_velocity\", count(\"row_id\").over(window_agg))  # Count tx per hour\n",
    "df_featured = df_featured.withColumn(\"amt_per_hour\", avg(\"Amount\").over(window_agg))  # Avg amount per hour\n",
    "\n",
    "# Other transformations \n",
    "df_featured = df_featured.withColumn(\"is_weekend\", \n",
    "                                    ((floor(col(\"Time\") / 3600) % 24).isin([0, 6, 12, 18])).cast(\"int\"))  # Peak hours flag (0=midnight,6=6am,12=noon,18=6pm; adjust bins if weekends)\n",
    "df_featured = df_featured.withColumn(\"v_sum\", col(\"V1\") + col(\"V2\") + col(\"V3\"))  # Simple PCA sum\n",
    "\n",
    "# Global z score: Use dummy partition for dataset wide mean/stddev\n",
    "global_window = Window.partitionBy(lit(1))  \n",
    "df_featured = df_featured.withColumn(\"amount_zscore\", \n",
    "                                    (col(\"Amount\") - avg(\"Amount\").over(global_window)) / \n",
    "                                    (stddev(\"Amount\").over(global_window) + lit(1e-6)))  \n",
    "\n",
    "# Cache for downstream efficiency\n",
    "df_featured.cache()\n",
    "df_featured.count()  \n",
    "\n",
    "print(\"Feature engineering complete! New columns added: hour_bin, tx_velocity, amt_per_hour, is_weekend, v_sum, amount_zscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53057f59-1ca7-41f7-92ec-94c9007e4539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema (new cols at end):\n",
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- row_id: long (nullable = true)\n",
      " |-- Log_Amount: double (nullable = true)\n",
      " |-- Hour_of_Day: double (nullable = true)\n",
      " |-- Amount_Category: string (nullable = true)\n",
      " |-- hour_bin: long (nullable = true)\n",
      " |-- tx_velocity: long (nullable = false)\n",
      " |-- amt_per_hour: double (nullable = true)\n",
      " |-- is_weekend: integer (nullable = true)\n",
      " |-- v_sum: double (nullable = true)\n",
      " |-- amount_zscore: double (nullable = true)\n",
      "\n",
      "\n",
      "Sample rows with new features:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 18:39:39 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+-----------------+----------+--------------------+--------------------+-----+\n",
      "|Time  |hour_bin|tx_velocity|amt_per_hour     |is_weekend|v_sum               |amount_zscore       |Class|\n",
      "+------+--------+-----------+-----------------+----------+--------------------+--------------------+-----+\n",
      "|3421.0|0       |82922      |65.91032693375992|1         |1.750654137015734   |-0.2835590273260809 |0    |\n",
      "|2802.0|0       |82922      |65.91032693375992|1         |1.25000803300733    |-0.34818900510264644|0    |\n",
      "|3345.0|0       |82922      |65.91032693375992|1         |2.083606484906916   |0.013221830623907898|0    |\n",
      "|2941.0|0       |82922      |65.91032693375992|1         |1.107401566872141   |-0.34818900510264644|0    |\n",
      "|366.0 |0       |82922      |65.91032693375992|1         |1.322487964381769   |0.03549431527306276 |0    |\n",
      "|2532.0|0       |82922      |65.91032693375992|1         |1.758934130712333   |-0.33033124508930617|0    |\n",
      "|3308.0|0       |82922      |65.91032693375992|1         |-0.22451286588283897|-0.24068449437645775|0    |\n",
      "|2179.0|0       |82922      |65.91032693375992|1         |2.24312552704176    |-0.31601321924342085|0    |\n",
      "|1316.0|0       |82922      |65.91032693375992|1         |-0.179119475338672  |0.28419046975728685 |0    |\n",
      "|836.0 |0       |82922      |65.91032693375992|1         |2.250953947872365   |-0.31239394048793323|0    |\n",
      "+------+--------+-----------+-----------------+----------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Per-class feature stats:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 18:39:40 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----------------+--------------------+-----------------+\n",
      "|Class|   avg_tx_per_hour| avg_amt_per_hour|         mean_zscore|       std_zscore|\n",
      "+-----+------------------+-----------------+--------------------+-----------------+\n",
      "|    0|153629.35389505472|88.54972400406585|-2.69639724865191...|0.999806162942079|\n",
      "|    1|130295.08499257058|86.10998381851581|  0.1599391993425607|1.097461130260703|\n",
      "+-----+------------------+-----------------+--------------------+-----------------+\n",
      "\n",
      "\n",
      "Global z score check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 18:39:44 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|         global_mean|        global_std|\n",
      "+--------------------+------------------+\n",
      "|-1.18386473877399...|0.9999999960228415|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validate\n",
    "\n",
    "print(\"Updated schema (new cols at end):\")\n",
    "df_featured.printSchema()\n",
    "\n",
    "print(\"\\nSample rows with new features:\")\n",
    "df_featured.select(\"Time\", \"hour_bin\", \"tx_velocity\", \"amt_per_hour\", \"is_weekend\", \"v_sum\", \"amount_zscore\", \"Class\").show(10, truncate=False)\n",
    "\n",
    "# Aggregate insights \n",
    "print(\"\\nPer-class feature stats:\")\n",
    "df_featured.groupBy(\"Class\").agg(\n",
    "    avg(\"tx_velocity\").alias(\"avg_tx_per_hour\"),\n",
    "    avg(\"amt_per_hour\").alias(\"avg_amt_per_hour\"),\n",
    "    mean(\"amount_zscore\").alias(\"mean_zscore\"),  \n",
    "    stddev(\"amount_zscore\").alias(\"std_zscore\")  \n",
    ").show()\n",
    "\n",
    "# Global z score check\n",
    "print(\"\\nGlobal z score check:\")\n",
    "df_featured.select(\n",
    "    mean(\"amount_zscore\").alias(\"global_mean\"),\n",
    "    stddev(\"amount_zscore\").alias(\"global_std\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405f10c-30ea-4e0c-bf05-95e21bb0571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 18:39:47 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n",
      "25/12/06 18:40:59 WARN MemoryStore: Not enough space to cache rdd_19_0 in memory! (computed 313.2 MiB so far)\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to featured_fraud_data.parquet (~5,998,034 rows, 41 cols)\n",
      "Saved schema matches: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save featured data\n",
    "output_path = \"featured_fraud_data.parquet\"\n",
    "df_featured.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(output_path)\n",
    "print(f\"Saved to {output_path} (~{df_featured.count():,} rows, {len(df_featured.columns)} cols)\")\n",
    "\n",
    "# Verify\n",
    "saved_df = spark.read.parquet(output_path)\n",
    "print(\"Saved schema matches:\", len(saved_df.columns) == len(df_featured.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
