{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92869a62b4275ab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:08:55.608777Z",
     "start_time": "2025-11-28T19:08:17.377838Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c439c69ee6b7954b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T19:12:47.090513Z",
     "start_time": "2025-11-28T19:12:44.412948Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/storage/home/tpk5410/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "25/12/06 17:56:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"Create Spark session with optimized configuration for data processing\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FraudDetection_DataAcquisition\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skew.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "        .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "\n",
    "spark = create_spark_session()\n",
    "print(\"Spark session created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f6bb10-b582-4865-8932-9b889c0c222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading credit card data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 284,807 rows\n",
      "Number of columns: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data Acquisition\n",
    "def load_credit_card_data(spark, filepath):\n",
    "    \"\"\"Load the credit card fraud dataset\"\"\"\n",
    "    print(\"Loading credit card data...\")\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Time\", DoubleType(), True),\n",
    "        StructField(\"V1\", DoubleType(), True),\n",
    "        StructField(\"V2\", DoubleType(), True),\n",
    "        StructField(\"V3\", DoubleType(), True),\n",
    "        StructField(\"V4\", DoubleType(), True),\n",
    "        StructField(\"V5\", DoubleType(), True),\n",
    "        StructField(\"V6\", DoubleType(), True),\n",
    "        StructField(\"V7\", DoubleType(), True),\n",
    "        StructField(\"V8\", DoubleType(), True),\n",
    "        StructField(\"V9\", DoubleType(), True),\n",
    "        StructField(\"V10\", DoubleType(), True),\n",
    "        StructField(\"V11\", DoubleType(), True),\n",
    "        StructField(\"V12\", DoubleType(), True),\n",
    "        StructField(\"V13\", DoubleType(), True),\n",
    "        StructField(\"V14\", DoubleType(), True),\n",
    "        StructField(\"V15\", DoubleType(), True),\n",
    "        StructField(\"V16\", DoubleType(), True),\n",
    "        StructField(\"V17\", DoubleType(), True),\n",
    "        StructField(\"V18\", DoubleType(), True),\n",
    "        StructField(\"V19\", DoubleType(), True),\n",
    "        StructField(\"V20\", DoubleType(), True),\n",
    "        StructField(\"V21\", DoubleType(), True),\n",
    "        StructField(\"V22\", DoubleType(), True),\n",
    "        StructField(\"V23\", DoubleType(), True),\n",
    "        StructField(\"V24\", DoubleType(), True),\n",
    "        StructField(\"V25\", DoubleType(), True),\n",
    "        StructField(\"V26\", DoubleType(), True),\n",
    "        StructField(\"V27\", DoubleType(), True),\n",
    "        StructField(\"V28\", DoubleType(), True),\n",
    "        StructField(\"Amount\", DoubleType(), True),\n",
    "        StructField(\"Class\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "\n",
    "    df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "\n",
    "    print(f\"Original dataset size: {df.count():,} rows\")\n",
    "    print(f\"Number of columns: {len(df.columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the data (replace with your actual file path)\n",
    "file_path = \"creditcard.csv\"  # Update this path\n",
    "df_original = load_credit_card_data(spark, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2c1b72-77f3-498a-94c3-15d4b629ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA CLEANING (Pre-Scaling) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 17:58:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean data first for synthetic scaling\n",
    "def clean_dataset(df):\n",
    "    \"\"\"Perform data cleaning operations on raw data\"\"\"\n",
    "    print(\"\\n=== DATA CLEANING (Pre-Scaling) ===\")\n",
    "    \n",
    "    # Remove duplicates from raw data first\n",
    "    original_count = df.count()\n",
    "    df_clean = df.dropDuplicates()\n",
    "    print(f\"Duplicates removed: {original_count - df_clean.count()}\")\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df_clean = df_clean.withColumn(\"Class\", F.col(\"Class\").cast(IntegerType()))\n",
    "    df_clean = df_clean.withColumn(\"Amount\", F.col(\"Amount\").cast(DoubleType()))\n",
    "    \n",
    "    # Handle potential outliers in Amount\n",
    "    amount_stats = df_clean.approxQuantile(\"Amount\", [0.001, 0.5, 0.999], 0.05)\n",
    "    cap_value = amount_stats[2]\n",
    "    df_clean = df_clean.withColumn(\n",
    "        \"Amount\", \n",
    "        F.when(F.col(\"Amount\") > cap_value, cap_value).otherwise(F.col(\"Amount\"))\n",
    "    )\n",
    "    return df_clean\n",
    "\n",
    "df_cleaned = clean_dataset(df_original) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdaf3e9-7083-4c94-8790-281ecb79bca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SYNTHETIC SCALING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original clean size: 283,726\n",
      "Scaling factor: 21.15x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled dataset size: 5,998,034 rows\n",
      "\n",
      "=== DATA QUALITY CHECKS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Row Count: 5,998,034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Synthetic Scaling \n",
    "def scale_dataset(df, target_rows=6000000, seed=42):\n",
    "    \"\"\"Scale dataset using bootstrapping\"\"\"\n",
    "    print(\"\\n=== SYNTHETIC SCALING ===\")\n",
    "    original_count = df.count()\n",
    "    scaling_factor = target_rows / original_count\n",
    "    \n",
    "    print(f\"Original clean size: {original_count:,}\")\n",
    "    print(f\"Scaling factor: {scaling_factor:.2f}x\")\n",
    "    \n",
    "    # Apply bootstrapping\n",
    "    df_scaled = df.sample(withReplacement=True, fraction=scaling_factor, seed=seed)\n",
    "    \n",
    "    # Add row_id to make rows distinct technically (helps downstream)\n",
    "    df_scaled = df_scaled.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "    \n",
    "    print(f\"Scaled dataset size: {df_scaled.count():,} rows\")\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "df_scaled = scale_dataset(df_cleaned, target_rows=6000000)\n",
    "\n",
    "def perform_data_quality_checks(df):\n",
    "    print(\"\\n=== DATA QUALITY CHECKS ===\")\n",
    "    # (Your original check logic here is fine, just run it on df_scaled)\n",
    "    print(f\"Current Row Count: {df.count():,}\")\n",
    "    return df\n",
    "\n",
    "df_final_check = perform_data_quality_checks(df_scaled)\n",
    "\n",
    "# ensure variable consistency for next steps\n",
    "df_cleaned = df_final_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d6a874-2886-4aab-a388-a121217224f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASIC FEATURE ENGINEERING ===\n",
      "Basic features created: Log_Amount, Hour_of_Day, Amount_Category\n"
     ]
    }
   ],
   "source": [
    "# Basic Feature Engineering (Foundation for Division 3)\n",
    "def create_basic_features(df):\n",
    "    \"\"\"\n",
    "    Create basic feature transformations\n",
    "    While Division 3 handles advanced features, we provide foundational transformations\n",
    "    \"\"\"\n",
    "    print(\"\\n=== BASIC FEATURE ENGINEERING ===\")\n",
    "\n",
    "    # Log transform Amount to handle skewness\n",
    "    # Adding 1 to avoid log(0) issues\n",
    "    df_featured = df.withColumn(\"Log_Amount\", F.log(F.col(\"Amount\") + 1))\n",
    "\n",
    "    # Create time-based features (hour of day from Time column)\n",
    "    # Assuming Time is in seconds, convert to hours modulo 24\n",
    "    df_featured = df_featured.withColumn(\"Hour_of_Day\", (F.col(\"Time\") / 3600) % 24)\n",
    "\n",
    "    # Create amount categories for basic segmentation\n",
    "    df_featured = df_featured.withColumn(\n",
    "        \"Amount_Category\",\n",
    "        F.when(F.col(\"Amount\") < 10, \"Very Low\")\n",
    "         .when(F.col(\"Amount\") < 50, \"Low\")\n",
    "         .when(F.col(\"Amount\") < 100, \"Medium\")\n",
    "         .when(F.col(\"Amount\") < 500, \"High\")\n",
    "         .otherwise(\"Very High\")\n",
    "    )\n",
    "\n",
    "    print(\"Basic features created: Log_Amount, Hour_of_Day, Amount_Category\")\n",
    "\n",
    "    return df_featured\n",
    "\n",
    "df_final = create_basic_features(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872a6703-f18a-4f30-b069-806cd1e6cc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL DATASET VALIDATION ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Final row count: 5,998,034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Fraud: 10,095 rows (0.1683%)\n",
      "2. Non-Fraud: 5,987,939 rows (99.8317%)\n",
      "3. Final schema:\n",
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- row_id: long (nullable = false)\n",
      " |-- Log_Amount: double (nullable = true)\n",
      " |-- Hour_of_Day: double (nullable = true)\n",
      " |-- Amount_Category: string (nullable = false)\n",
      "\n",
      "4. Data sample:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+------------------+------+-----------------+-----+\n",
      "|Time|              V1|                V2|Amount|       Log_Amount|Class|\n",
      "+----+----------------+------------------+------+-----------------+-----+\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "|73.0|1.14818692615291|0.0858370715501502| 19.77|3.033509637888021|    0|\n",
      "+----+----------------+------------------+------+-----------------+-----+\n",
      "only showing top 10 rows\n",
      "5. Final null check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------\n",
      " Time            | 0   \n",
      " V1              | 0   \n",
      " V2              | 0   \n",
      " V3              | 0   \n",
      " V4              | 0   \n",
      " V5              | 0   \n",
      " V6              | 0   \n",
      " V7              | 0   \n",
      " V8              | 0   \n",
      " V9              | 0   \n",
      " V10             | 0   \n",
      " V11             | 0   \n",
      " V12             | 0   \n",
      " V13             | 0   \n",
      " V14             | 0   \n",
      " V15             | 0   \n",
      " V16             | 0   \n",
      " V17             | 0   \n",
      " V18             | 0   \n",
      " V19             | 0   \n",
      " V20             | 0   \n",
      " V21             | 0   \n",
      " V22             | 0   \n",
      " V23             | 0   \n",
      " V24             | 0   \n",
      " V25             | 0   \n",
      " V26             | 0   \n",
      " V27             | 0   \n",
      " V28             | 0   \n",
      " Amount          | 0   \n",
      " Class           | 0   \n",
      " row_id          | 0   \n",
      " Log_Amount      | 0   \n",
      " Hour_of_Day     | 0   \n",
      " Amount_Category | 0   \n",
      "\n",
      "\n",
      "Dataset validation: PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Data Validation before Storage\n",
    "def validate_final_dataset(df):\n",
    "    \"\"\"Comprehensive validation of the final dataset\"\"\"\n",
    "    print(\"\\n=== FINAL DATASET VALIDATION ===\")\n",
    "    \n",
    "    final_count = df.count()\n",
    "    print(f\"1. Final row count: {final_count:,}\")\n",
    "    \n",
    "    # check fraud distribution\n",
    "    fraud_dist = df.groupBy(\"Class\").count().collect()\n",
    "    for row in fraud_dist:\n",
    "        class_type = \"Fraud\" if row['Class'] == 1 else \"Non-Fraud\"\n",
    "        percentage = (row['count'] / final_count) * 100\n",
    "        print(f\"2. {class_type}: {row['count']:,} rows ({percentage:.4f}%)\")\n",
    "    \n",
    "    # verify schema\n",
    "    print(\"3. Final schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "\n",
    "    print(\"4. Data sample:\")\n",
    "    df.select(\"Time\", \"V1\", \"V2\", \"Amount\", \"Log_Amount\", \"Class\").show(10)\n",
    "    \n",
    "\n",
    "    print(\"5. Final null check:\")\n",
    "    \n",
    "    # identify numeric columns for isnan check\n",
    "    numeric_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType, FloatType, IntegerType, LongType))]\n",
    "    \n",
    "\n",
    "    null_exprs = []\n",
    "    for c in df.columns:\n",
    "        if c in numeric_cols:\n",
    "            # For numeric, check both NaN and Null\n",
    "            expr = F.sum(F.when(F.isnan(c) | F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        else:\n",
    "            # For strings/others, just check Null\n",
    "            expr = F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        null_exprs.append(expr)\n",
    "            \n",
    "    null_summary = df.select(null_exprs)\n",
    "    null_summary.show(vertical=True)\n",
    "    \n",
    "    return final_count >= 5000000  # minimum size requirement\n",
    "# validate dataset\n",
    "is_valid = validate_final_dataset(df_final)\n",
    "print(f\"\\nDataset validation: {'PASSED' if is_valid else 'FAILED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9043b43c-9132-476c-b0dc-9d67ca3dfdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SAVING OPTIMIZED DATASET ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n",
      "Writing Parquet files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully saved to: cleaned_fraud_data.parquet\n",
      "Verified saved data: 5,998,034 rows\n"
     ]
    }
   ],
   "source": [
    "# Optimization and Storage\n",
    "def save_optimized_dataset(df, output_path=\"cleaned_fraud_data.parquet\"):\n",
    "    \"\"\"\n",
    "    Save the optimized dataset in Parquet format\n",
    "\n",
    "    Why Parquet?\n",
    "    - Columnar storage: Like a zip file for columns, making reads much faster\n",
    "    - Compression: Reduces storage space significantly\n",
    "    - Predicate pushdown: Filters data before reading\n",
    "    - Compatible with Spark MLlib\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== SAVING OPTIMIZED DATASET ===\")\n",
    "\n",
    "    # Repartition for optimal distributed processing\n",
    "    # 10 partitions for better parallelism\n",
    "    df_optimized = df.repartition(10)\n",
    "\n",
    "    print(f\"Number of partitions: {df_optimized.rdd.getNumPartitions()}\")\n",
    "\n",
    "    # Save as Parquet with Snappy compression\n",
    "    print(\"Writing Parquet files...\")\n",
    "    df_optimized.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", \"snappy\") \\\n",
    "        .parquet(output_path)\n",
    "\n",
    "    print(f\"Dataset successfully saved to: {output_path}\")\n",
    "\n",
    "    # Verify the saved data\n",
    "    verify_df = spark.read.parquet(output_path)\n",
    "    saved_count = verify_df.count()\n",
    "    print(f\"Verified saved data: {saved_count:,} rows\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "output_path = save_optimized_dataset(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56348379-6ec3-414d-8c97-7f4737cd522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA ACQUISITION & PREPROCESSING SUMMARY REPORT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 284,807 rows\n",
      "Final dataset size:    5,998,034 rows\n",
      "Scaling factor:        21.06x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud Distribution:\n",
      "  Original: 492 fraud cases (0.1727%)\n",
      "  Final:    10,095 fraud cases (0.1683%)\n",
      "\n",
      "Output Details:\n",
      "  Format:    Parquet (Snappy compression)\n",
      "  Location:  cleaned_fraud_data.parquet\n",
      "  Partitions: 4\n",
      "\n",
      "Features Available for Next Division:\n",
      "  Total features: 34\n",
      "  New features: Log_Amount, Hour_of_Day, Amount_Category\n",
      "\n",
      "============================================================\n",
      "\n",
      "Pipeline completed successfully!\n",
      "\n",
      "Instructions for Division 2 (EDA):\n",
      "1. Load the data: df = spark.read.parquet('cleaned_fraud_data.parquet')\n",
      "2. Verify data: df.count(), df.printSchema()\n",
      "3. Begin EDA with the provided clean dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5998034"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Generate Summary Report\n",
    "def generate_summary_report(df_original, df_final, output_path):\n",
    "    \"\"\"Generate a comprehensive summary of the preprocessing pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA ACQUISITION & PREPROCESSING SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    original_count = df_original.count()\n",
    "    final_count = df_final.count()\n",
    "\n",
    "    print(f\"Original dataset size: {original_count:,} rows\")\n",
    "    print(f\"Final dataset size:    {final_count:,} rows\")\n",
    "    print(f\"Scaling factor:        {final_count/original_count:.2f}x\")\n",
    "\n",
    "    # Fraud statistics\n",
    "    original_fraud = df_original.filter(F.col(\"Class\") == 1).count()\n",
    "    final_fraud = df_final.filter(F.col(\"Class\") == 1).count()\n",
    "\n",
    "    print(f\"\\nFraud Distribution:\")\n",
    "    print(f\"  Original: {original_fraud:,} fraud cases ({original_fraud/original_count*100:.4f}%)\")\n",
    "    print(f\"  Final:    {final_fraud:,} fraud cases ({final_fraud/final_count*100:.4f}%)\")\n",
    "\n",
    "    print(f\"\\nOutput Details:\")\n",
    "    print(f\"  Format:    Parquet (Snappy compression)\")\n",
    "    print(f\"  Location:  {output_path}\")\n",
    "    print(f\"  Partitions: {df_final.rdd.getNumPartitions()}\")\n",
    "\n",
    "    print(f\"\\nFeatures Available for Next Division:\")\n",
    "    features = [col for col in df_final.columns if col not in ['row_id']]\n",
    "    print(f\"  Total features: {len(features)}\")\n",
    "    print(f\"  New features: Log_Amount, Hour_of_Day, Amount_Category\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "    # generate final report\n",
    "generate_summary_report(df_original, df_final, output_path)\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")\n",
    "print(\"\\nInstructions for Division 2 (EDA):\")\n",
    "print(\"1. Load the data: df = spark.read.parquet('cleaned_fraud_data.parquet')\")\n",
    "print(\"2. Verify data: df.count(), df.printSchema()\")\n",
    "print(\"3. Begin EDA with the provided clean dataset\")\n",
    "\n",
    "\n",
    "# spark.stop()\n",
    "\n",
    "spark.read.parquet(\"cleaned_fraud_data.parquet\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
